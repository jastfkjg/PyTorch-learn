{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Module 是其核心数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.autograd import Variable as V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0027, -0.4393,  3.7245],\n",
       "        [-3.3274, -0.7227,  0.5685]], grad_fn=<ThAddBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Linear(nn.Module):   #继承nn.Module\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = nn.Parameter(t.randn(in_features, out_features))   #nn.Parameter是一种特殊的Variable,默认需要求导\n",
    "        self.b = nn.Parameter(t.randn(out_features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.mm(self.w)\n",
    "        return x + self.b.expand_as(x)\n",
    "    \n",
    "layer = Linear(4, 3)       #全连接层\n",
    "input = V(t.randn(2, 4))\n",
    "output = layer(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w Parameter containing:\n",
      "tensor([[-1.0120,  0.0892,  0.1106],\n",
      "        [ 0.2262,  0.6317,  0.4206],\n",
      "        [ 0.5001, -0.4838,  1.0983],\n",
      "        [-1.8336,  0.5101, -1.7111]], requires_grad=True)\n",
      "b Parameter containing:\n",
      "tensor([-0.5480, -0.4312,  1.7770], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in layer.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Parameter??    #查看Parameter类的源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多层感知机（两个全连接层，采用sigmoid函数作为激活函数）\n",
    "class Perceptron(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layer1 = Linear(in_features, hidden_features)\n",
    "        self.layer2 = Linear(hidden_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = t.sigmoid(x)\n",
    "        return self.layer2(x)\n",
    "    \n",
    "perceptron = Perceptron(3, 4, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，pytorch实现了神经网络中绝大多数的layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.AvgPool2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Linear??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.BatchNorm2d??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential 是一个特殊的Module, 包含几个子module，前向传播时会将输入一层接一层传递下去。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1:  Sequential(\n",
      "  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation_layer): ReLU()\n",
      ")\n",
      "net2:  Sequential(\n",
      "  (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      ")\n",
      "net3:  Sequential(\n",
      "  (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Sequential 的三种写法\n",
    "\n",
    "net1 = nn.Sequential()\n",
    "net1.add_module('conv', nn.Conv2d(3, 3, 3))\n",
    "net1.add_module('batchnorm', nn.BatchNorm2d(3))\n",
    "net1.add_module('activation_layer', nn.ReLU())      #激活函数可作为独立的layer使用\n",
    "\n",
    "net2 = nn.Sequential(nn.Conv2d(3, 3, 3), nn.BatchNorm2d(3), nn.ReLU())\n",
    "\n",
    "from collections import OrderedDict\n",
    "net3 = nn.Sequential(OrderedDict([('conv1', nn.Conv2d(3, 3, 3)), ('bn1', nn.BatchNorm2d(3)), ('relu1', nn.ReLU())]))\n",
    "\n",
    "print('net1: ', net1)\n",
    "print('net2: ', net2)\n",
    "print('net3: ', net3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)),\n",
       " BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU())"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net1.conv, net2[1], net3.relu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN  RNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5134, -0.2633, -0.0370],\n",
       "         [-0.3588,  0.2899,  0.1300],\n",
       "         [-0.1729, -0.1179,  0.0504]],\n",
       "\n",
       "        [[-0.5915, -0.0094,  0.2564],\n",
       "         [-0.3588,  0.3240,  0.1656],\n",
       "         [-0.1866, -0.0089,  0.0474]]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = V(t.randn(2, 3, 4))\n",
    "lstm = nn.LSTM(4, 3, 1)\n",
    "h0 = V(t.randn(1, 3, 3))\n",
    "c0 = V(t.randn(1, 3, 3))\n",
    "out, hn = lstm(input, (h0, c0))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1346,  0.1886, -0.1895],\n",
       "         [ 0.1065, -0.2896,  0.2246],\n",
       "         [-0.1018,  0.1794, -0.0827]],\n",
       "\n",
       "        [[-0.1695,  0.1479,  0.0184],\n",
       "         [-0.0086,  0.0190, -0.0218],\n",
       "         [ 0.0877,  0.2225,  0.1486]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = V(t.randn(2, 3, 4))\n",
    "lstm = nn.LSTMCell(4, 3)\n",
    "hx = V(t.randn(3, 3))\n",
    "cx = V(t.randn(3, 3))\n",
    "out = []\n",
    "for i in input:\n",
    "    hx, cx = lstm(i, (hx, cx))\n",
    "    out.append(hx)\n",
    "t.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.stack??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(2, 2), nn.Conv2d(6, 16, 5), nn.ReLU(), nn.MaxPool2d(2, 2))\n",
    "        self.classifier = nn.Sequential(nn.Linear(16*5*5, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim \n",
    "optimizer = optim.SGD(params = net.parameters(), lr=1)\n",
    "optimizer.zero_grad()   #等价与net.zero_grad()\n",
    "\n",
    "input = V(t.randn(1, 3, 32, 32))\n",
    "output = net(input)\n",
    "output.backward(output)\n",
    "\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需调整学习率， 可新建一个optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([{'params': net.features.parameters()}, {'params': net.classifier.parameters(), 'lr': 0.01}], lr = 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = V(t.randn(2, 3))\n",
    "\n",
    "model = nn.Linear(3, 4)\n",
    "output1 = model(input)\n",
    "\n",
    "output2 = nn.functional.linear(input, model.weight, model.bias)\n",
    "\n",
    "output1 == output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = nn.functional.relu(input)\n",
    "b2 = nn.ReLU()(input)\n",
    "b == b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数的初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.init模块实现了常用的初始化策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.6411, -0.5984,  0.0877],\n",
       "        [ 0.7913, -0.2122,  0.9752],\n",
       "        [-0.0548,  0.3535, -0.5248],\n",
       "        [-0.2230,  0.8966,  0.5706]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "linear = nn.Linear(3, 4)\n",
    "\n",
    "init.xavier_normal_(linear.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Module??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存模型\n",
    "t.save(net.state_dict(), 'net.pth')\n",
    "\n",
    "#加载保存的模型\n",
    "net2 =Net()\n",
    "net2.load_state_dict(t.load('net.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#将模型放到GPU上运行，只需：\n",
    "\n",
    "model = model.cuda()\n",
    "\n",
    "input.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch as t\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(nn.Conv2d(inchannel, outchannel, 3, stride, 1, bias =False),\n",
    "                                 nn.BatchNorm2d(outchannel), nn.ReLU(inplace=True),\n",
    "                                 nn.Conv2d(outchannel, outchannel, 3, 1, 1, bias =False),\n",
    "                                 nn.BatchNorm2d(outchannel))\n",
    "        self.right = shortcut\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.pre = nn.Sequential(nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n",
    "                                nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "                                nn.MaxPool2d(3, 2, 1))\n",
    "        self.layer1 = self._make_layer(64, 128, 3)\n",
    "        self.layer2 = self._make_layer(128, 256, 4, stride = 2)\n",
    "        self.layer3 = self._make_layer(256, 512, 6, stride = 2)\n",
    "        self.layer4 = self._make_layer(512, 512, 3, stride = 2)\n",
    "        \n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, inchannel, outchannel, block_num, stride = 1):\n",
    "        \n",
    "        shortcut = nn.Sequential(nn.Conv2d(inchannel, outchannel, 1, stride, bias=False), \n",
    "                                nn.BatchNorm2d(outchannel))\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))\n",
    "        \n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock(outchannel, outchannel))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1894, -0.3569,  0.3082,  0.4709, -0.1184,  0.5463,  0.1248, -0.2602,\n",
       "          0.3080, -0.4432,  0.0798, -0.3327,  0.2541, -0.4049, -0.0502,  0.1631,\n",
       "         -0.0280, -0.0694,  0.1476, -0.4899, -0.3459,  0.0744, -0.5930,  0.5776,\n",
       "          0.1642,  0.5396,  0.0666, -0.1787, -0.1806, -0.2015,  0.0343,  0.1598,\n",
       "         -0.1636, -0.4603, -0.1124,  0.1034,  0.5849,  0.0224, -0.0573,  0.0337,\n",
       "          0.3540, -0.7250, -0.0857, -0.1482, -0.3968,  0.5321, -0.4246,  0.2845,\n",
       "         -0.4286,  0.3081,  0.7063,  0.4696,  0.0683,  0.0688, -0.5384, -0.0954,\n",
       "          0.0485, -0.0693, -0.1764,  0.1659,  0.2950, -0.6973,  0.4064,  0.7254,\n",
       "          0.0145,  0.5843, -0.0018, -0.0244,  0.6111, -0.2469, -0.6280, -0.2148,\n",
       "         -0.5064,  0.0384,  0.4206, -0.4653,  0.0713, -0.1483,  0.0898, -0.2034,\n",
       "          0.4141, -0.0493, -0.1261,  0.8232, -0.0769,  0.2669, -0.0972,  0.0413,\n",
       "          0.0911,  0.1049, -0.1080,  0.2174,  0.2858, -0.0021, -0.1397,  0.3337,\n",
       "          0.1673,  0.2258,  0.0886, -0.0323,  0.0372,  0.6307,  0.5074, -0.2358,\n",
       "          0.1448,  0.3553,  0.0960, -0.1698,  0.5693,  0.0071,  0.8496,  0.2493,\n",
       "          0.1574, -0.6806,  0.1059, -0.0146,  0.2259,  0.3888, -0.5355, -0.4609,\n",
       "         -0.2024, -0.0437,  0.2895,  0.1530,  0.1116, -0.0461,  0.6300, -0.0462,\n",
       "          0.4022,  0.5224, -0.3166, -0.4781,  0.1604, -0.3276,  0.2780, -0.2255,\n",
       "         -0.9648, -0.3684,  0.1978,  0.2337, -0.1329,  0.3994, -0.0710,  0.0001,\n",
       "          0.1072, -0.1833,  0.0583, -0.2025,  0.5534,  0.2353,  0.0278, -0.3414,\n",
       "          0.1011,  0.0513, -0.5706, -0.2010, -0.1898,  0.0964,  0.1217, -0.1887,\n",
       "         -0.5055,  0.6187,  0.1129, -0.5735,  0.0133,  0.4832, -0.1622, -0.0531,\n",
       "          0.3315, -0.4830,  0.1555,  0.0167,  0.4177,  0.1514, -0.2484, -0.4536,\n",
       "         -0.4195, -0.9260,  0.2489, -0.0151,  0.0396, -0.4929, -0.2547, -0.1442,\n",
       "         -0.3204,  0.5461,  0.2051, -0.0802,  0.7754, -0.4108,  0.4370,  0.2957,\n",
       "          0.1640, -0.2074,  0.1562,  0.2144, -0.2546, -0.1794, -0.2198,  0.3351,\n",
       "         -0.6367,  0.2365, -0.2903, -0.1550,  0.6045, -0.3275,  0.3121, -0.5230,\n",
       "         -0.3832, -0.1473,  0.3897, -0.5376, -0.2756, -0.4581,  0.6040,  0.1158,\n",
       "         -0.3592,  0.0327, -0.4995, -0.2523,  0.0797, -0.9043, -0.7059, -0.2089,\n",
       "         -0.1013, -0.0428, -0.0838,  0.2159,  0.7385, -0.0017, -0.6636,  0.3033,\n",
       "         -0.4090, -0.6060,  0.0227,  0.2023,  0.1058,  0.1119,  0.0642, -0.0213,\n",
       "         -0.2738,  0.3573, -0.1737, -0.7214, -0.4377, -0.2400,  0.0169,  0.1414,\n",
       "          0.0299, -0.0943,  0.0223, -0.1582,  0.1229,  0.6763,  0.0629,  0.3179,\n",
       "          0.0140,  0.1825,  0.0686,  0.3877, -0.4035,  0.0042,  0.3714,  0.1756,\n",
       "         -0.0463,  0.1054, -0.2153, -0.2437,  0.6084, -0.4212, -0.0264, -0.0736,\n",
       "          0.3579, -0.2833,  0.6335, -0.3965,  0.5021, -0.0467,  0.0893, -0.0617,\n",
       "          0.4447,  0.0458,  0.2369, -0.2286, -0.2156,  0.0551, -0.0181,  0.1951,\n",
       "          0.0002, -0.1081,  0.4214, -0.3284,  0.0144,  0.1144,  0.0827,  0.1090,\n",
       "         -0.3263,  0.2140,  0.0729, -0.1806, -0.6637, -0.3004, -0.0162,  0.5655,\n",
       "         -0.1920,  0.5070,  0.2285,  0.0619, -0.1090,  0.0351, -0.3848, -0.2913,\n",
       "          0.2000,  0.2754, -0.0689, -0.1427,  0.3827, -0.3394, -0.4812, -0.0799,\n",
       "          0.1589,  0.4293, -0.1821, -0.0384, -0.2208, -0.0070, -0.2615,  0.1184,\n",
       "         -0.1585,  0.6000, -0.1248,  0.1218,  0.2507, -0.5710, -0.0895,  0.6446,\n",
       "          0.3353,  0.1524, -0.1998, -0.4017,  0.2130, -0.1918, -0.1663,  0.1624,\n",
       "         -0.1785,  0.0775,  0.3157,  0.4684,  0.9340,  0.4374,  0.1109,  0.3533,\n",
       "          0.6234, -0.1305, -0.2388,  0.5753,  0.5474, -0.2651, -0.3600,  0.1873,\n",
       "         -0.5831, -0.1133, -0.1017, -0.2576,  0.0468,  0.2267, -0.4229, -0.3394,\n",
       "         -1.0651, -0.4083,  0.9082,  0.2414, -0.0430,  0.0539,  0.0629,  0.4223,\n",
       "          0.4314, -0.1617,  0.3044,  0.0933,  0.1745,  0.0363, -0.6367,  0.4733,\n",
       "          0.1870, -0.2718, -0.5552,  0.2215, -0.2284, -0.0642, -0.0397,  0.0055,\n",
       "          0.1357,  0.3637,  0.3245,  0.0430,  0.0451, -0.2342, -0.6175,  0.0860,\n",
       "          0.3159, -0.1771,  0.1741, -0.0756, -0.0557, -0.1543, -0.1268,  0.4733,\n",
       "          0.0387, -0.4673,  0.4276,  0.5031,  0.6509,  0.1896, -0.4267,  0.2657,\n",
       "         -0.3511,  0.4002,  0.0184, -0.1267,  0.1729, -0.4379,  0.4690, -0.0350,\n",
       "          0.7985,  0.3161,  0.0222, -0.2726, -0.5237,  0.0783,  0.5024, -0.3115,\n",
       "         -0.0008, -0.7157, -0.1899, -0.0149,  0.1474, -0.5939,  0.2939,  0.0891,\n",
       "          0.4018, -0.4520, -0.1368, -0.0072,  0.3155, -0.4208, -0.1308, -0.2229,\n",
       "          0.2413, -0.3458, -0.1309, -0.6112,  0.1514,  0.3345, -0.7715, -0.3917,\n",
       "         -0.3611, -0.3128,  0.1487, -0.8214,  0.1489, -0.2233,  0.0344,  0.7214,\n",
       "         -0.0415,  0.0540,  0.1331, -0.3211,  0.2011,  0.3277, -0.3214,  0.2410,\n",
       "         -0.4542, -0.3991,  0.0866,  0.0741, -0.1252,  0.1305,  0.2612, -0.1591,\n",
       "         -0.1097,  0.2837,  0.0708,  0.1117,  0.0024,  0.1402,  0.0547,  0.2840,\n",
       "          0.2323,  0.2686,  0.6884,  0.1795, -0.3927, -0.5741,  0.2705, -0.2934,\n",
       "         -0.0717,  0.4097,  0.1604, -0.3191, -0.3071, -0.0971, -0.2800, -0.0011,\n",
       "         -0.0370,  0.9937,  0.2599,  0.1171, -0.3255,  0.5399, -0.5153, -0.6922,\n",
       "         -0.0139, -0.9199,  0.1268,  0.1254,  0.1531,  0.2595,  0.4864, -0.0705,\n",
       "         -0.2456, -0.1437, -0.0821,  0.0174, -0.2736, -0.0866,  0.1366, -0.1919,\n",
       "          0.3664,  0.5564, -0.6995,  0.0786, -0.0783,  0.1129,  0.7419,  0.1180,\n",
       "         -0.3913, -0.6188,  0.1252,  0.0554,  0.5053,  0.1812, -0.0824, -0.0428,\n",
       "          0.4876,  0.3017,  0.3515, -0.5164, -0.0780, -0.2013, -0.1486,  0.4129,\n",
       "          0.4613,  0.2237,  0.3336,  0.0145, -0.2868,  0.0958,  0.0414, -0.0159,\n",
       "          0.1368, -0.0827,  0.5285,  0.4117, -0.1504, -0.1719, -0.2466,  0.7544,\n",
       "          0.1969,  0.3343, -0.6804, -0.0833, -0.4778, -0.0139,  0.1430, -0.0781,\n",
       "          0.1704,  0.2116,  0.2002, -0.6018,  0.2665, -0.4719,  0.0198, -0.3953,\n",
       "          0.1128, -0.2296, -0.5245,  0.3991, -0.5068,  0.1276,  0.0589,  0.6436,\n",
       "         -0.0036,  1.0350,  0.4255, -0.2907,  0.1924, -0.1411, -0.2893, -0.2586,\n",
       "         -0.2470, -0.1162,  0.0107, -0.0063,  0.6238, -0.4999,  0.1442, -0.1875,\n",
       "          0.2164, -0.1461,  0.0165,  0.3225, -0.2162, -0.2970, -0.4068, -0.4542,\n",
       "         -0.3644,  0.2741,  0.1372, -0.2721,  0.1730, -0.0756, -0.5209, -0.0418,\n",
       "         -0.2369,  0.4930,  0.1865,  0.7608,  0.5982,  0.3557,  0.2978,  0.0242,\n",
       "          0.0246, -0.3001, -0.2914, -0.2123, -0.1668, -0.0286,  0.0328,  0.0433,\n",
       "         -0.0144,  0.0192,  0.2622, -0.1552, -0.3751, -0.2952, -0.0052,  0.2810,\n",
       "          0.5701,  0.0812,  0.1316,  0.0422, -0.5178,  0.6315,  0.4673,  0.1496,\n",
       "         -0.1002, -0.4560, -0.0680,  0.2090,  0.2652, -0.1102, -0.6368,  0.1049,\n",
       "         -0.2512, -0.4863,  0.2037, -0.1644, -0.9048, -0.6067,  0.3072,  0.1063,\n",
       "         -0.0882, -0.2937, -0.3296,  0.4870, -0.2571,  0.3895, -0.0502, -0.3332,\n",
       "          0.0166, -0.1556, -0.2617,  0.0185,  0.3871, -0.2278,  0.2126,  0.0450,\n",
       "         -0.4774,  0.1012,  0.4262, -0.1221,  0.0216, -0.2075, -0.1235,  0.1554,\n",
       "          0.2154, -0.3590,  0.5947,  0.2304,  0.4754,  0.2478,  0.1579,  0.2015,\n",
       "          0.2049, -0.0763, -0.4880,  0.3147, -0.6564, -0.1361,  0.0801,  0.2160,\n",
       "          0.1453,  0.2502, -0.4137,  0.2882, -0.0473,  0.1370,  0.5306,  0.3415,\n",
       "          0.5071,  0.1736,  0.1463,  0.1242, -0.1030,  0.5334, -0.4567, -0.3458,\n",
       "         -0.1561, -0.2694, -0.3040,  0.2897, -0.1378, -0.2014,  0.6604, -0.4040,\n",
       "          0.6865, -0.4689,  0.5792,  0.5111,  0.1650, -0.4311, -0.2802,  0.6966,\n",
       "         -0.2563,  0.2676,  0.2663,  0.0150,  0.3581,  0.1077,  0.2362,  0.1651,\n",
       "          0.1522, -0.4497,  0.0766,  0.4509,  0.4041, -0.2145,  0.0443, -0.3877,\n",
       "          0.4313,  0.0168,  0.3839, -0.3311,  0.3037, -0.1063,  0.1802, -0.4500,\n",
       "         -0.1152, -0.3420, -0.1575,  0.1376,  0.4137, -0.3056, -0.8971, -0.0892,\n",
       "         -0.8747, -0.0218, -0.2804, -0.2517,  0.0661, -0.2894, -0.6420, -0.3825,\n",
       "         -0.3500,  0.5800, -0.1451,  0.3126,  0.2551,  0.0790, -0.1214,  0.3579,\n",
       "         -0.2789,  0.0324, -0.1580,  0.1387, -0.3481, -0.3053, -0.0638,  0.0174,\n",
       "          0.0275,  0.0599, -0.0804, -0.2144,  0.2550,  0.0791, -0.1062,  0.2498,\n",
       "         -0.1191,  1.0568, -0.2081,  0.2140,  0.0817, -0.2478, -0.1518, -0.1811,\n",
       "          0.0349, -0.1875, -0.0741,  0.1308, -0.1470,  0.2132, -0.0909,  0.2493,\n",
       "          0.3679, -0.7130, -0.5959,  0.0441, -0.1519, -0.2446,  0.0577, -0.3865,\n",
       "          0.0945, -0.9014, -0.3868, -0.2481, -0.2495, -0.3504,  0.0144, -0.9148,\n",
       "         -0.0401,  0.2984, -0.2379,  0.0063, -0.1387,  0.0824,  0.2458,  0.1926,\n",
       "         -0.0489, -0.1169,  0.5349,  0.0816, -0.5446,  0.0880, -0.0596,  0.2565,\n",
       "          0.2717,  0.2755,  0.2882, -0.4158,  0.1029,  0.5539, -0.7928,  0.1078,\n",
       "          0.1546, -0.1638,  0.8186, -0.2329,  0.8188,  0.1602,  0.6118,  0.8236,\n",
       "         -0.3457, -0.3424,  0.1957,  0.2152,  0.2811, -0.0553,  0.4466,  0.1066,\n",
       "          0.4929,  0.3527, -0.0529,  0.4512,  0.3563,  0.3968, -0.3860, -0.1723,\n",
       "         -0.3960, -0.2630,  0.4375,  0.3393,  0.5349,  0.6231, -0.2998, -0.2292,\n",
       "         -0.0794,  0.0359,  0.0338,  0.5259,  0.0882, -0.3248,  0.3702,  0.4107,\n",
       "          0.2740, -0.2296,  0.5815,  0.0778, -0.7543,  0.3589,  0.7322, -0.0906,\n",
       "          0.1020,  0.0147, -0.0004, -0.0460,  1.0148,  0.8690,  0.0833, -0.2992,\n",
       "         -0.5843, -0.1986, -0.9113,  0.2829,  0.0923,  0.5464, -0.2024,  0.0213,\n",
       "          0.0406, -0.2667, -0.5308,  0.1950,  0.4425, -0.6292,  0.1701,  0.3458,\n",
       "          0.3099,  0.0788, -0.4191, -0.1305, -0.1279, -0.0240, -0.0849, -0.4857,\n",
       "          0.5099, -0.2214, -0.1796,  0.2684,  0.2742, -0.5951,  0.7227, -0.0210,\n",
       "         -0.1570,  0.2702, -0.2727,  0.4899, -0.5051,  0.1973, -0.1604,  0.2702,\n",
       "          0.0287, -0.1700, -0.0668, -0.1680, -0.9261, -0.1206, -0.0647,  0.4127,\n",
       "         -0.7417,  0.1005, -0.0221,  0.3220,  0.3886, -0.3824, -0.3807, -0.2156,\n",
       "          0.4346,  0.1237, -0.3009, -0.3330, -0.1328, -0.2654, -0.2421,  0.2108,\n",
       "         -0.1038,  0.1888, -0.1551,  0.3950, -0.0748, -0.0796,  0.5269, -0.5201,\n",
       "          0.1989, -0.0188,  0.2076, -0.2173, -0.1451,  0.0088,  0.0938,  0.4450]],\n",
       "       grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet()\n",
    "input = t.autograd.Variable(t.randn(1, 3, 224, 224))\n",
    "out = model(input)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
