{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some pratice of using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 中的Autograd模块用于提供微分，帮助反向传播求导。autograd.Variable 是核心类，封装了Tensor. 并包含三个属性：\n",
    "1. data\n",
    "2. grad\n",
    "3. grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = Variable(t.ones(2,2), requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SumBackward0 at 0x7f0561ad3828>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.sum()\n",
    "print(y)\n",
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [4., 4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每次运行后，梯度都会累加到之前的梯度，故反向传播之前需要把他们清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.data.zero_()\n",
    "y.backward()\n",
    "x.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn 是专门为神经网络设计的模块化接口，构建于Autograd之上。 nn.module 是重要的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "<bound method Module.parameters of Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "print(net)\n",
    "print(net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight :  torch.Size([6, 1, 5, 5])\n",
      "conv1.bias :  torch.Size([6])\n",
      "conv2.weight :  torch.Size([16, 6, 5, 5])\n",
      "conv2.bias :  torch.Size([16])\n",
      "fc1.weight :  torch.Size([120, 400])\n",
      "fc1.bias :  torch.Size([120])\n",
      "fc2.weight :  torch.Size([84, 120])\n",
      "fc2.bias :  torch.Size([84])\n",
      "fc3.weight :  torch.Size([10, 84])\n",
      "fc3.bias :  torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    print(name, ': ', param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在输入时需要把Tensor 封装成Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "tensor([[ 0.0782, -0.0188,  0.1065,  0.0996,  0.1524, -0.0084, -0.1102, -0.0115,\n",
      "          0.0647,  0.0476]], grad_fn=<ThAddmmBackward>)\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([ 0.0329,  0.0211, -0.0486,  0.0416, -0.0195,  0.0405])\n"
     ]
    }
   ],
   "source": [
    "input = Variable(t.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out.size())\n",
    "print(out)\n",
    "\n",
    "net.zero_grad()  \n",
    "out.backward(Variable(t.ones(1, 10)))\n",
    "print('--'*40)\n",
    "print(net.conv1.bias.grad)   #计算反向传播后conv1.bias的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数  \n",
    "nn.MSELoss 来计算均方误差， nn.CrossEntropyLoss 来计算交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4176, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "\n",
    "target = Variable(t.rand(1,10))\n",
    "criterion = nn.MSELoss()        #nn.MSELoss 是一个class,需先实例化\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0011, -0.0002,  0.0037, -0.0020,  0.0036, -0.0005])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "loss.backward()\n",
    "print(net.conv1.bias.grad)   #根据所选损失函数计算反向传播后conv1.bias的导数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器\n",
    "根据损失函数计算完梯度后，还需选择优化方法 来更新网络中的参数和权重。 如SGD：\n",
    "\n",
    "weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.date.sub_(f.grad.data * learning_rate)      #手动实现SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.optim 中实现了深度学习中绝大多数的优化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.1853, -0.1959, -0.1373,  0.0577, -0.1046,  0.0327], requires_grad=True)\n",
      "--------------------------------------------------------------------------------\n",
      "Parameter containing:\n",
      "tensor([ 0.1853, -0.1959, -0.1373,  0.0577, -0.1046,  0.0327], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)  #lr为学习率\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(net.conv1.bias)\n",
    "\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()  #更新参数\n",
    "print('--'*40)\n",
    "print(net.conv1.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
